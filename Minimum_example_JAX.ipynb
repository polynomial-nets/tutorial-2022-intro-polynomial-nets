{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBZQNjLTy6qV"
   },
   "source": [
    "This notebook is providing a minimum code for running MNIST classification with a CCP model (i.e. a polynomial expansion without activation functions) in *JAX*. \n",
    "\n",
    "*Details*: The model implements a third-degree polynomial expansion (and in particular the [CCP model](https://github.com/grigorisg9gr/polynomial_nets) from the $\\Pi$-Nets), using a hidden dimension of 32. The linear operations involved are implemented as convolutions. The code is inspired by this tutorial: https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen (and was verified with JAX version 0.3.25).\n",
    "\n",
    "For implementations that obtain state-of-the-art code with polynomial nets, please visit other respositories, such as the https://github.com/grigorisg9gr/polynomial_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bymClH6LzLrU",
    "outputId": "660c4ffd-9108-4b37-86d6-bf6b44ebffbf"
   },
   "outputs": [],
   "source": [
    "# # Install the dependencies\n",
    "!pip install --upgrade -q pip jax jaxlib flax optax tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrDe_c03zoiJ"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp               # JAX NumPy\n",
    "from flax.linen import Dense\n",
    "from flax import linen as nn          # The Linen API\n",
    "from flax.training import train_state\n",
    "import optax                          # The Optax gradient processing and optimization library\n",
    "from jax import jit, random, device_get, tree_map, value_and_grad\n",
    "\n",
    "import numpy as np                    # Ordinary NumPy\n",
    "import tensorflow_datasets as tfds    # TFDS for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzD4O-JIzx6m"
   },
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    # Split into training/test sets\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    # Convert to floating-points\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.0\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.0\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_n5C_8bz2hU",
    "outputId": "9da22ea5-c9b9-410d-efa4-4c1f9a9bc110"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pi_net_Convs(nn.Module):\n",
    "\n",
    "  @nn.compact\n",
    "  # Provide a constructor to register a new parameter \n",
    "  # and return its initial value\n",
    "  def __call__(self, x):\n",
    "    x1 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x2 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x3 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    \n",
    "    x = x1 * x2 * x3 + x1 * x2 + x2 * x3 + x1\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1)) # Flatten\n",
    "    x = nn.Dense(features=32)(x)\n",
    "\n",
    "    x = nn.Dense(features=10)(x)    # There are 10 classes in MNIST\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "  loss = jnp.mean(optax.softmax_cross_entropy(logits, nn.one_hot(labels, num_classes=10)))\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "  def loss_fn(params):\n",
    "    logits = Pi_net_Convs().apply({'params': params}, batch['image'])\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(\n",
    "        logits=logits, \n",
    "        labels=nn.one_hot(batch['label'], num_classes=10)))\n",
    "    return loss, logits\n",
    "\n",
    "  grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "  (_, logits), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = compute_metrics(logits, batch['label'])\n",
    "  return state, metrics\n",
    "\n",
    "@jit\n",
    "def eval_step(params, batch):\n",
    "  logits = Pi_net_Convs().apply({'params': params}, batch['image'])\n",
    "  return compute_metrics(logits, batch['label'])\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = random.permutation(rng, len(train_ds['image']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # Skip an incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  batch_metrics = []\n",
    "\n",
    "  for perm in perms:\n",
    "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "    state, metrics = train_step(state, batch)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  training_batch_metrics = device_get(batch_metrics)\n",
    "  training_epoch_metrics = {\n",
    "      k: np.mean([metrics[k] for metrics in training_batch_metrics])\n",
    "      for k in training_batch_metrics[0]}\n",
    "\n",
    "  print('Training - epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, training_epoch_metrics['loss'], training_epoch_metrics['accuracy'] * 100))\n",
    "\n",
    "  return state, training_epoch_metrics\n",
    "\n",
    "\n",
    "\n",
    "def eval_model(model, test_ds):\n",
    "  metrics = eval_step(model, test_ds)\n",
    "  metrics = device_get(metrics)\n",
    "  eval_summary = tree_map(lambda x: x.item(), metrics)\n",
    "  return eval_summary['loss'], eval_summary['accuracy']\n",
    "\n",
    "train_ds, test_ds = get_datasets()\n",
    "rng = random.PRNGKey(0)\n",
    "rng, init_rng = random.split(rng)\n",
    "cnn = Pi_net_Convs()\n",
    "params = cnn.init(init_rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "nesterov_momentum = 0.9\n",
    "learning_rate = 0.001\n",
    "tx = optax.sgd(learning_rate=learning_rate, nesterov=nesterov_momentum)\n",
    "\n",
    "state = train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = random.split(rng)\n",
    "  # Run an optimization step over a training batch\n",
    "  state, train_metrics = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "  print('Testing - epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpYe6t8K0H8K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
