{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is providing a minimum code for running MNIST classification with a CCP model (i.e. a polynomial expansion without activation functions). \n",
        "\n",
        "Details: The model implements a third-degree expansion, using a hidden dimension of 32. The linear operations involved are implemented as convolutions. The code is inspired by this tutorial: https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen (and was verified with JAX version 0.3.25)."
      ],
      "metadata": {
        "id": "mBZQNjLTy6qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install the dependencies\n",
        "!pip install --upgrade -q pip jax jaxlib flax optax tensorflow-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bymClH6LzLrU",
        "outputId": "660c4ffd-9108-4b37-86d6-bf6b44ebffbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp               # JAX NumPy\n",
        "from flax.linen import Dense\n",
        "from flax import linen as nn          # The Linen API\n",
        "from flax.training import train_state\n",
        "import optax                          # The Optax gradient processing and optimization library\n",
        "from jax import jit, random, device_get, tree_map, value_and_grad\n",
        "\n",
        "import numpy as np                    # Ordinary NumPy\n",
        "import tensorflow_datasets as tfds    # TFDS for MNIST"
      ],
      "metadata": {
        "id": "IrDe_c03zoiJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_datasets():\n",
        "    ds_builder = tfds.builder('mnist')\n",
        "    ds_builder.download_and_prepare()\n",
        "    # Split into training/test sets\n",
        "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
        "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
        "    # Convert to floating-points\n",
        "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.0\n",
        "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.0\n",
        "    return train_ds, test_ds"
      ],
      "metadata": {
        "id": "IzD4O-JIzx6m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pi_net_Convs(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  # Provide a constructor to register a new parameter \n",
        "  # and return its initial value\n",
        "  def __call__(self, x):\n",
        "    x1 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x2 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x3 = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    \n",
        "    x = x1 * x2 * x3 + x1 * x2 + x2 * x3 + x1\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1)) # Flatten\n",
        "    x = nn.Dense(features=32)(x)\n",
        "\n",
        "    x = nn.Dense(features=10)(x)    # There are 10 classes in MNIST\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "  loss = jnp.mean(optax.softmax_cross_entropy(logits, nn.one_hot(labels, num_classes=10)))\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "\n",
        "@jit\n",
        "def train_step(state, batch):\n",
        "  def loss_fn(params):\n",
        "    logits = Pi_net_Convs().apply({'params': params}, batch['image'])\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(\n",
        "        logits=logits, \n",
        "        labels=nn.one_hot(batch['label'], num_classes=10)))\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits, batch['label'])\n",
        "  return state, metrics\n",
        "\n",
        "@jit\n",
        "def eval_step(params, batch):\n",
        "  logits = Pi_net_Convs().apply({'params': params}, batch['image'])\n",
        "  return compute_metrics(logits, batch['label'])\n",
        "\n",
        "\n",
        "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]  # Skip an incomplete batch\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  batch_metrics = []\n",
        "\n",
        "  for perm in perms:\n",
        "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
        "    state, metrics = train_step(state, batch)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  training_batch_metrics = device_get(batch_metrics)\n",
        "  training_epoch_metrics = {\n",
        "      k: np.mean([metrics[k] for metrics in training_batch_metrics])\n",
        "      for k in training_batch_metrics[0]}\n",
        "\n",
        "  print('Training - epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, training_epoch_metrics['loss'], training_epoch_metrics['accuracy'] * 100))\n",
        "\n",
        "  return state, training_epoch_metrics\n",
        "\n",
        "\n",
        "\n",
        "def eval_model(model, test_ds):\n",
        "  metrics = eval_step(model, test_ds)\n",
        "  metrics = device_get(metrics)\n",
        "  eval_summary = tree_map(lambda x: x.item(), metrics)\n",
        "  return eval_summary['loss'], eval_summary['accuracy']\n",
        "\n",
        "train_ds, test_ds = get_datasets()\n",
        "rng = random.PRNGKey(0)\n",
        "rng, init_rng = random.split(rng)\n",
        "cnn = Pi_net_Convs()\n",
        "params = cnn.init(init_rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "nesterov_momentum = 0.9\n",
        "learning_rate = 0.001\n",
        "tx = optax.sgd(learning_rate=learning_rate, nesterov=nesterov_momentum)\n",
        "\n",
        "state = train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
        "\n",
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  # Use a separate PRNG key to permute image data during shuffling\n",
        "  rng, input_rng = random.split(rng)\n",
        "  # Run an optimization step over a training batch\n",
        "  state, train_metrics = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
        "  # Evaluate on the test set after each training epoch\n",
        "  test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
        "  print('Testing - epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_n5C_8bz2hU",
        "outputId": "9da22ea5-c9b9-410d-efa4-4c1f9a9bc110"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - epoch: 1, loss: 0.7278, accuracy: 82.28\n",
            "Testing - epoch: 1, loss: 0.39, accuracy: 89.39\n",
            "Training - epoch: 2, loss: 0.3639, accuracy: 89.99\n",
            "Testing - epoch: 2, loss: 0.32, accuracy: 90.99\n",
            "Training - epoch: 3, loss: 0.3191, accuracy: 91.27\n",
            "Testing - epoch: 3, loss: 0.29, accuracy: 92.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpYe6t8K0H8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}